---
title: "Introduction to openalexR: a webinar"
date: today
author: Trang Le
format: 
  html:
    theme: 
      light: sandstone
      dark: slate
    toc: true
    toc_float: true
    code-link: true
    code-tools:
      source: https://github.com/trangdata/openalexR-webinar
      toggle: false
---

## üåª Introduction

<img src="images/logo.png" align="right" height="139"/>

Welcome to the webinar on **openalexR**!
Today, we will explore how to use the **openalexR** package to fetch data from OpenAlex, a free and open database of the entire research landscape.
With **openalexR**, you can easily access and analyze scholarly data directly from R.

## üå± Installation and setup

If you haven't already installed the **openalexR** package, you can do so from CRAN:

```{r eval=FALSE}
#| label: install
#| eval: false
#| 
install.packages("openalexR")
```

Before we go any further, we highly recommend you set the `openalexR.mailto` option so that your requests go to [the polite pool](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication#the-polite-pool) for faster response times.
If you have OpenAlex Premium, you can add your API key to the `openalexR.apikey` option as well.
To do so, you can open `.Renviron` with `file.edit("~/.Renviron")` and add:

```         
openalexR.mailto = example@email.com
openalexR.apikey = EXAMPLE_APIKEY
```

We will now load in **openalexR** and the [**tidyverse**](https://www.tidyverse.org/) to use for the rest of this webinar.

```{r}
#| label: load
#| message: false

library(openalexR)
library(tidyverse)
packageVersion("openalexR")
```

```{r}
#| label: set-theme
#| include: false

theme_set(theme_classic())
theme_update(
  plot.background = element_rect(fill = "transparent", colour = NA),
  panel.background = element_rect(fill = "transparent", colour = NA),
  legend.title = element_blank(),
  legend.position = c(0.7, 0.4),
)

knitr::opts_chunk$set(dev.args = list(bg = "transparent"))
```

::: {.callout-note collapse="true"}
## R base pipe

Throughout the webinar, you will see this symbol, `|>`.
Simply put, `x |> f()` is equivalent to `f(x)`.
Using this [R base pipe](https://developer.r-project.org/Blog/public/2021/02/21/a-native-pipe-in-r-2021-02-21/) allows us to chain functions together in a more readable way.
:::

## üåø Basic usage

The main function of **openalexR** is `oa_fetch()`.

```{r}
#| eval: false

?oa_fetch
```

### Fetching information from identifiers

Let's start by fetching data on a specific scholarly work using its OpenAlex ID:

::: {.callout-note collapse="true"}
When you know the OpenAlex ID of the work, you do not need to specify `entity = "works"` because entity can be inferred from the first character of `id`.
However, I specify it here for clarity.
In other use cases (search, filter), you will almost always want to specify `entity`.
For a list of all supported entities, see `oa_entities()`.
:::

```{r fetch-work}
work <- oa_fetch(entity = "works", id = "W2741809807", verbose = TRUE)
work
```

Now, we can view the output tibble/dataframe, `work`, interactively in RStudio or inspect it with base functions like `str` or `head`.

```{r}
str(work, max.level = 2)
```

**openalexR** also provides the `show_works` function to simplify the result (e.g., remove some columns, keep first/last author) for easy viewing.
Let us define `print_oa()` to wrap the output table in `knitr::kable()` to be displayed nicely on the webpage, but you will most likely not need this function.

```{r}
print_oa <- function(x, fun = show_works) {
  x |>
    select(-any_of("url")) |>
    fun() |>
    knitr::kable() |>
    identity()
}
print_oa(work)
```

### Filter and search

There are different [filters](https://ropensci.github.io/openalexR/articles/Filters)/arguments you can use in `oa_fetch()`, depending on which [entity](https://docs.openalex.org/#data) you're interested in: works, authors, sources, funders, institutions, or concepts.
We show a few examples below.

#### 1. Use `doi` as a [works filter](https://ropensci.github.io/openalexR/articles/Filters.html#works)

**Goal**: Fetch information on two works with DOIs `10.1016/j.joi.2017.08.007` and `https://doi.org/10.1007/s11192-013-1221-3`.
[^1]

[^1]: Either canonical form with <https://doi.org/> or without will work.

```{r}
works_from_dois <- oa_fetch(
  entity = "works",
  doi = c("10.1016/j.joi.2017.08.007", "https://doi.org/10.1007/s11192-013-1221-3"),
  verbose = TRUE
)
print_oa(works_from_dois)
```

#### 2. Use `author.orcid` as a filter

**Goal**: Fetch all works by authors with ORCID IDs `0000-0001-6187-6610` and `0000-0002-8517-9411`.
[^2]

[^2]: Either canonical form with <https://orcid.org/> or without will work.

```{r}
works_from_orcids <- oa_fetch(
  entity = "works",
  author.orcid = c("0000-0001-6187-6610", "https://orcid.org/0000-0002-8517-9411"),
  verbose = TRUE
)

print_oa(works_from_orcids)
```

#### 3. Many filters at once

**Goal**: Download all works that have been cited more than 50 times, published between 2020 and 2021, and include the strings "bibliometric analysis" or "science mapping" in the title.
Maybe we also want the results to be sorted by total citations in a descending order.

```{r}
works_biblio <- oa_fetch(
  entity = "works",
  title.search = c("bibliometric analysis", "science mapping"),
  cited_by_count = ">100",
  from_publication_date = "2020-07-01",
  to_publication_date = "2021-07-01",
  options = list(sort = "cited_by_count:desc")
)

print_oa(works_biblio)
```

#### 4. What if we use a wrong filter?

**Goal**: Sample 10 works published in 2024.

Say we mistakenly use `year` instead of `publication_year` as a filter.

```{r}
#| error: true

oa_fetch(
  entity = "works",
  year = 2024,
  options = list(sample = 10),
  verbose = TRUE
)
```

The API returns a helpful error message: `year` is a wrong filter, and shows you all possible, correct filters.
In our case, `publication_year` is the filter we want.

```{r}
oa_fetch(
  entity = "works",
  publication_year = 2024,
  options = list(sample = 10, seed = 1),
  verbose = TRUE
) |>
  print_oa()
```

#### 5. Search

You can also search for works related to a specific topic.
Here's an example fetching the first few works of which abstracts include the phrase "urban heat island":

```{r search-works}
uhi <- oa_fetch(
  entity = "works",
  abstract.search = "urban heat island",
  options = list(sample = 10, seed = 1)
)

print_oa(uhi)
```

::: {.callout-tip collapse="true"}
You can read more about search [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/search-entities).
<!-- You will learn how relevance score is calculated, how words are stemmed to improve search results, and how to do complex boolean searches. --> Specifically, stop words like "the" and "an" are removed, and, by default, [stemming](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-kstem-tokenfilter.html) is used to improve results ("possums" will also return records with the word "possum").
To disable stemming and the removal of stop words for searches on titles and abstracts, you can add `.no_stem` to the search filter, *e.g.* `abstract.search.no_stem`.
:::

#### 6. Other entities

Similar to works, you can fetch information on authors, sources, funders, institutions, and concepts.
Here are some examples of fetching authors:

**Goal**: Acquire information on a couple of authors with more than 10 works.
Here, we can use filters such as `display_name` and `works_count`:

```{r}
oa_fetch(
  entity = "authors",
  display_name.search = c("Massimo Aria", "Kyle Demes"),
  works_count = ">10"
) |>
  print_oa(show_authors)
```

## üõ§Ô∏è Coding challenges

### Challenge 1: Advanced filters

According to the OpenAlex [API documentation](https://docs.openalex.org/api-entities), what is the filter we should use to:

-   Get `funders` with a description containing "engineering"?
-   Get `topics` with more than 1000 works?
-   Get 10 `institutions` located in Asia?

::: panel-tabset
## Your solution

```{r }
#| label: challenge-filters
#| eval: false

oa_fetch(
  entity = "funders",
  _____ = "engineering"
)

oa_fetch(
  entity = "topics",
  _____ = ">1000"
)

oa_fetch(
  entity = "institutions",
  _____ = "Asia",
  options = list(_____ = 10)
)
```

## My solution

```{r}
#| label: answer-filters
#| eval: false
#| 
oa_fetch(
  entity = "funders",
  description.search = "engineering"
)

oa_fetch(
  entity = "topics",
  works_count = ">1000"
)

oa_fetch(
  entity = "institutions",
  continent = "Asia",
  options = list(sample = 10, seed = 1)
)
```
:::

### Challenge 2: Humpback whale

Identify works on a specific topic (e.g., "humpback whale") that have been cited more than 100 times.
When were these works published?
Where are the authors based?
Create a bar plot showing the number of works at each institution.

::: panel-tabset
## Your solution

```{r}
#| label: challenge-top-authors
#| eval: false

work <- oa_fetch(
  entity = "works",
  # decide whether you want search/title.search/abstract.search/etc.
  ______
)

______
```

## My solution

```{r answer-top-authors}
humpback <- oa_fetch(
  entity = "works",
  title.search = "humpback whale",
  cited_by_count = ">100",
  options = list(sort = "cited_by_count:desc")
)
print(humpback$title[1:10])
n_authors <- sapply(humpback$author, nrow)
hb_authors <- humpback$author |>
  bind_rows() |> 
  mutate(weight = 1/unlist(lapply(n_authors, \(x) rep(x, x))))

pal <- c("#D3B484", "#F89C74", "#C9DB74", "#87C55F", "#B497E7","#66C5CC")
hb_authors |>
  drop_na(institution_display_name) |>
  group_by(
    inst = institution_display_name, 
    country = institution_country_code
  ) |>
  summarise(n = sum(weight),  .groups = "drop") |> 
  arrange(desc(n)) |> 
  filter(n > 0.5) |>
  ggplot() +
  aes(x = n, y = fct_reorder(inst, n), fill = country) +
  geom_col() +
  scale_fill_manual(values = rev(pal)) +
  coord_cartesian(expand = FALSE) +
  labs(
    x = "Weighted value of most cited works",
    y = NULL,
    title = "Institutions with most cited works on humpback whale"
  )
```
:::

## ü™¥ Example analyses

### Heterocycles authors

How many authors published in Heterocycles?

LIVE DEMO

### Prolific authors

**Goal**: Analyze the open access status of datasets published by researchers at [Dartmouth college](https://openalex.org/institutions/i107672454) over the years.

```{r}
#| fig-height: 4
#| fig-width: 8
#| 
w_dartmouth <- oa_fetch(
  "works",
  authorships.institutions.lineage = "i107672454",
  publication_year = ">2000",
  publication_year = "<2024",
  type = "dataset",
  options = list(
    select = c("id", "open_access", "publication_year")
  #   sample = 500, seed = 1 # sample for a faster response
  ),
  # count_only = TRUE,
  verbose = TRUE
)
w_dartmouth

w_dartmouth |> 
  mutate(
    oa_status = oa_status %>% 
      fct_relevel("gold", "hybrid", "bronze", "green", "closed") |> 
      fct_recode(
        "Gold: Published in an OA journal" = "gold",
        "Green: Toll-access on publisher page, but free copy in an OA repository" = "green",
        "Bronze: Free to read on publisher page, but no identifiable license" = "bronze",
        "Hybrid: Free under an open license in a toll-access journal" = "hybrid",
        "Closed" = "closed"
      )
  ) |>
  count(publication_year, oa_status) |> 
  ggplot() +
  aes(y = as.factor(publication_year), x = n, fill = oa_status) +
  geom_col() +
  scale_fill_manual(values = c("#FFD700", "#1E90FF", "#CD7F32", "#32CD32", "#A9A9A9")) +
  labs(
    x = "Number of datasets", y = NULL,
    # title = "Open access status of datasets published by Dartmouth researchers",
    ) +
  coord_cartesian(expand = FALSE) 

```

### Journal clocks

**Goal**: Visualize big journals' topics.

We first download all records of journals with the most works/citation count, then visualize their scored concepts:

```{r big-journals}
jours_all <- oa_fetch(
  entity = "sources",
  works_count = ">200000",
  verbose = TRUE
)
```

The following is a lot of code but it is mainly for processing the data and customizing the final plot.

```{r}
#| label: visualize-big-journals
#| fig-height: 8
#| fig-width: 8
#| code-fold: true

clean_journal_name <- function(x) {
  x |>
    gsub("\\(.*?\\)", "", x = _) |>
    gsub("Journal of the|Journal of", "J.", x = _) |>
    gsub("/.*", "", x = _)
}

jours <- jours_all |>
  filter(type == "journal") |>
  slice_max(cited_by_count, n = 9) |>
  distinct(display_name, .keep_all = TRUE) |>
  select(jour = display_name, topics) |>
  tidyr::unnest(topics) |>
  filter(name == "field") |>
  group_by(id, jour, display_name) |> 
  summarise(score = (sum(count))^(1/3), .groups = "drop") |> 
  left_join(concept_abbrev, by = join_by(id, display_name)) |>
  mutate(
    abbreviation = gsub(" ", "<br>", abbreviation),
    jour = clean_journal_name(jour),
  ) |>
  tidyr::complete(jour, abbreviation, fill = list(score = 0)) |>
  group_by(jour) |>
  mutate(
    color = if_else(score > 10, "#1A1A1A", "#D9D9D9"), # CCCCCC
    label = paste0("<span style='color:", color, "'>", abbreviation, "</span>")
  ) |>
  ungroup()

jours |>
  ggplot() +
  aes(fill = jour, y = score, x = abbreviation, group = jour) +
  facet_wrap(~jour) +
  geom_hline(yintercept = c(25, 50), colour = "grey90", linewidth = 0.2) +
  geom_segment(
    aes(x = abbreviation, xend = abbreviation, y = 0, yend = 55),
    color = "grey95"
  ) +
  geom_col(color = "grey20") +
  coord_polar(clip = "off") +
  theme_bw() +
  theme(
    plot.background = element_rect(fill = "transparent", colour = NA),
    panel.background = element_rect(fill = "transparent", colour = NA),
    panel.grid = element_blank(),
    panel.border = element_blank(),
    axis.text = element_blank(),
    axis.ticks.y = element_blank()
  ) +
  ggtext::geom_richtext(
    aes(y = 75, label = label),
    fill = NA, label.color = NA, size = 3
  ) +
  scale_fill_brewer(palette = "Set1", guide = "none") +
  labs(y = NULL, x = NULL, title = "Journal clocks")
```

## üåµ Advanced topics

### Other parameters of `oa_fetch()`

So far, we have seen the argument `options`, which is a list of additional parameters that can be passed to the API.
Some of these options include:

-   `select`: Top-level fields to show in output.[^3]

-   `sort`: Attribute to sort by, *e.g.*: "display_name" for sources or "cited_by_count:desc" for works.[^4]

-   `sample`: number of (random) records to return.[^5]

-   `seed` A seed value in order to retrieve the same set of random records in the same order when used multiple times with `sample`.

[^3]: <https://docs.openalex.org/how-to-use-the-api/get-single-entities/select-fields>

[^4]: <https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/sort-entity-lists>

[^5]: <https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/sample-entity-lists>

Another helpful argument is `output`.
By default, `oa_fetch()` returns a tibble with pre-specified columns that we think are useful for most users.
We're working on better documenting for this process in this [pull request](https://github.com/ropensci/openalexR/pull/211).
However, if you need more control over the output (e.g., you want more fields that are not returned in the dataframe), you can set `output = "list"` to get the raw JSON (R list) object returned by the API.
Please [make an issue](https://github.com/ropensci/openalexR/issues/new) if you think a field is important to return in the dataframe output.
[^6]

[^6]: [Example issue](https://github.com/ropensci/openalexR/issues/205)

### Building your own query

Behind the scene, `oa_fetch()` composes three functions below so the user can execute everything in one step, *i.e.*, `oa_query() |> oa_request() |> oa2df()`

-   `oa_query()`: generates a valid query, written following the OpenAlex API syntax, from a set of arguments provided by the user.

-   `oa_request()`: downloads a collection of entities matching the query created by `oa_query` or manually written by the user, and returns a JSON object in a list format.

-   `oa2df()`: converts the JSON object in classical bibliographic tibble/data frame.

Therefore, instead of using `oa_fetch()`, you can use `oa_query()` and `oa_request()` separately to build your own query and make an API request, then (optionally) convert the result to a dataframe using `oa2df()`.
This way, you have more control over the process.

### `oa_generate()`

`oa_generate()` is a generator for making request to OpenAlex API, returning one record at a time.
This is useful when you want to process a large number of records without loading them all into memory at once.
You will need to first build your own query (*e.g.*, using `oa_query()` or interactively from the interface provided directly by OpenAlex) to use as an argument in `oa_generate()`.

```{r attr.output='.details summary="Output"'}
example(oa_generate)
```

### `oa_snowball()`

The user can also perform *snowballing* with `oa_snowball()`.
Snowballing is a literature search technique where the researcher starts with a set of articles and find articles that cite or were cited by the original set.
`oa_snowball()` returns a list of 2 elements: *nodes* and *edges*.
Similar to `oa_fetch()`, `oa_snowball()` finds and returns information on a core set of articles satisfying certain criteria, but, unlike `oa_fetch()`, it also returns information the articles that cite and are cited by this core set.

### üåæ N-grams

OpenAlex offers (limited) support for [fulltext N-grams](https://docs.openalex.org/api-entities/works/get-n-grams#fulltext-coverage) of Work entities (these have IDs starting with `"W"`).
Given a vector of work IDs, `oa_ngrams()` returns a dataframe of N-gram data (in the `ngrams` list-column) for each work.

```{r}
#| label: ngrams
#| fig-height: 3
#| fig-width: 8
#| 
ngrams_data <- oa_ngrams(
  works_identifier = c("W1964141474", "W1963991285"),
  verbose = TRUE
)
ngrams_data

lapply(ngrams_data$ngrams, head, 3)

ngrams_data |>
  unnest(ngrams) |>
  filter(ngram_tokens == 2) |>
  select(id, ngram, ngram_count) |>
  group_by(id) |>
  slice_max(ngram_count, n = 10, with_ties = FALSE) |>
  ggplot(aes(ngram_count, fct_reorder(ngram, ngram_count))) +
  geom_col(aes(fill = id), show.legend = FALSE) +
  scale_fill_manual(values = c("#A16928", "#2887a1")) +
  facet_wrap(~id, scales = "free_y") +
  labs(
    title = "Top 10 fulltext bigrams",
    x = "Count",
    y = NULL
  )
```

`oa_ngrams()` can sometimes be slow because the N-grams data can get pretty big, but given that the N-grams are ["cached via CDN"](https://docs.openalex.org/api-entities/works/get-n-grams#api-endpoint), you may also consider parallelizing for this special case.[^7]

[^7]: `oa_ngrams()` parallelizes automatically if you have `{curl} >= v5.0.0`

## üå∏ Q&A

TODO: to fill after webinar

# üçÅ Conclusion

Thank you for participating in the **openalexR** webinar!
We hope you found the session informative and engaging.
For more information, visit the [openalexR GitHub page](https://github.com/ropensci/openalexR) reach out to me on the [OpenAlex Community](https://groups.google.com/g/openalex-community) Google Group with any questions.

------------------------------------------------------------------------

## üå≥ Links

-   <https://openalex.org>
-   <https://docs.openalex.org>
-   <https://docs.ropensci.org/openalexR>
